"""
DeconvGNN-Vis å¯è§†åŒ–ç³»ç»Ÿå…¥å£
è¯¥æ¨¡å—è´Ÿè´£æ„å»ºåŸºäº Streamlit çš„ Web ç•Œé¢ï¼ŒåŒ…æ‹¬æ•°æ®é›†ç®¡ç†ã€å®æ—¶å›¾è¡¨æ¸²æŸ“åŠäº¤äº’é€»è¾‘ã€‚
"""

import streamlit as st
import pandas as pd
import os
import logging
import re
import time
import math
import tracemalloc
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Optional, Any


# --- è·¨ç¯å¢ƒå¯¼å…¥é€‚é… (æ”¯æŒæœ¬åœ°å¼€å‘ä¸ Streamlit Cloud) ---
try:
    import visualization.styles as styles
    import visualization.data_loader as data_loader
    import visualization.viz_utils as utils
except ImportError:
    import styles
    import data_loader
    import viz_utils as utils

# --- é¡µé¢å…¨å±€é…ç½® ---
st.set_page_config(
    page_title="DeconvGNN-Vis",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="collapsed" # åˆå§‹æ”¶èµ·ä¾§è¾¹æ ä»¥å±•ç¤ºæ¬¢è¿é¡µ
)

def _get_env_int(key: str, default: int) -> int:
    try:
        value = int(os.getenv(key, default))
        return value if value > 0 else default
    except Exception:
        return default

def _get_logger() -> logging.Logger:
    logger = logging.getLogger("visualization.app")
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter("%(asctime)s %(levelname)s %(name)s %(message)s")
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    logger.setLevel(os.getenv("DECONV_VIS_LOG_LEVEL", "INFO").upper())
    return logger

logger = _get_logger()
MAX_UPLOAD_MB = _get_env_int("DECONV_VIS_MAX_UPLOAD_MB", 200)
DATASET_NAME_MAX_LEN = _get_env_int("DECONV_VIS_DATASET_NAME_MAX_LEN", 64)
MAX_PERF_RECORDS = _get_env_int("DECONV_VIS_MAX_PERF_RECORDS", 200)
SHOW_PERF_MONITOR_TAB = False

def normalize_dataset_name(name: str) -> str:
    cleaned = re.sub(r"[^\w\u4e00-\u9fff\- ]+", "_", name).strip()
    cleaned = cleaned[:DATASET_NAME_MAX_LEN]
    return cleaned if cleaned else "dataset"

def ensure_unique_dataset_name(name: str, existing: Dict[str, str]) -> str:
    if name not in existing:
        return name
    counter = 1
    while f"{name}_{counter}" in existing:
        counter += 1
    return f"{name}_{counter}"

def render_messages(errors: List[str], warnings: List[str]) -> None:
    for err in errors:
        st.error(f"âŒ {err}")
    for warn in warnings:
        st.warning(f"âš ï¸ {warn}")

def _init_perf_state() -> None:
    if "perf_metrics" not in st.session_state:
        st.session_state.perf_metrics = []
    if "perf_monitor_enabled" not in st.session_state:
        st.session_state.perf_monitor_enabled = True
    if "perf_mode" not in st.session_state:
        st.session_state.perf_mode = "é«˜æ€§èƒ½"
    if "parallel_load" not in st.session_state:
        st.session_state.parallel_load = True
    if "binary_cache" not in st.session_state:
        st.session_state.binary_cache = True
    if "prewarm_bg" not in st.session_state:
        st.session_state.prewarm_bg = True
    if "auto_prewarm" not in st.session_state:
        st.session_state.auto_prewarm = True
    if "summary_index" not in st.session_state:
        st.session_state.summary_index = True
    if "prewarm_pending" not in st.session_state:
        st.session_state.prewarm_pending = False
    if "prewarm_mode" not in st.session_state:
        st.session_state.prewarm_mode = "auto"
    if "prewarm_future" not in st.session_state:
        st.session_state.prewarm_future = None
    if "prewarm_notified" not in st.session_state:
        st.session_state.prewarm_notified = False
    if "mem_snapshot" not in st.session_state:
        st.session_state.mem_snapshot = None

def record_metric(label: str, duration_ms: float, extra: Optional[Dict[str, Any]] = None) -> None:
    if not st.session_state.get("perf_monitor_enabled"):
        return
    item = {
        "label": label,
        "duration_ms": round(duration_ms, 2),
        "mode": st.session_state.get("perf_mode", "æ ‡å‡†"),
        "ts": pd.Timestamp.now().strftime("%H:%M:%S")
    }
    if extra:
        item.update(extra)
    st.session_state.perf_metrics.append(item)
    if len(st.session_state.perf_metrics) > MAX_PERF_RECORDS:
        st.session_state.perf_metrics = st.session_state.perf_metrics[-MAX_PERF_RECORDS:]

def run_timed(label: str, fn, extra: Optional[Dict[str, Any]] = None):
    start = time.perf_counter()
    result = fn()
    duration = (time.perf_counter() - start) * 1000
    record_metric(label, duration, extra)
    return result

def get_perf_mode() -> str:
    return st.session_state.get("perf_mode", "æ ‡å‡†")


# æ³¨å…¥è‡ªå®šä¹‰æ ·å¼ï¼ˆå¼ºåˆ¶æŒ‰é’®ä¸æ¢è¡Œã€éšè—é»˜è®¤èœå•ç­‰ï¼‰
styles.inject_custom_css()



def main():
    """
    åº”ç”¨æ ¸å¿ƒå…¥å£å‡½æ•°ï¼Œæ§åˆ¶æ•´ä½“ä¸šåŠ¡é€»è¾‘ä¸ç•Œé¢æµè½¬ã€‚
    """
    _init_perf_state()
    if "figure_cache" not in st.session_state:
        st.session_state.figure_cache = {}
    
    # === ä¾§è¾¹æ åŒºåŸŸï¼šæ•°æ®æºç®¡ç† ===
    with st.sidebar:
        st.markdown('<p class="main-header">DeconvGNN-Vis<br>ç©ºé—´è½¬å½•ç»„åå·ç§¯<br>å¯è§†åŒ–ç³»ç»Ÿ</p>', unsafe_allow_html=True)
        st.divider()
        
        # ç³»ç»Ÿé‡ç½®å·¥å…·
        if st.button("âš¡ é‡ç½®ç³»ç»Ÿ", type="secondary", use_container_width=True, help="æ¸…é™¤æ‰€æœ‰ç¼“å­˜å¹¶é‡æ–°åŠ è½½åº”ç”¨"):
            st.cache_data.clear()
            st.rerun()
            
        st.header("æ•°æ®é›†ç®¡ç†", help="""ç›®æ ‡æ–‡ä»¶å¤¹å¿…é¡»åŒ…å«ï¼š  
            `predict_result.csv`  
            `coordinates.csv`""")
        
        # åˆå§‹åŒ–ä¼šè¯æ•°æ®æº
        if 'data_sources' not in st.session_state:
            st.session_state.data_sources = data_loader.DATA_DIRS.copy()
        
        if 'show_import' not in st.session_state:
            st.session_state.show_import = False
            
        # æ•°æ®é›†åˆ—è¡¨è·å–ä¸é€‰æ‹©é€»è¾‘
        options = list(st.session_state.data_sources.keys())
        
        # ------------------- ä¾§è¾¹æ é€»è¾‘ï¼šç©ºçŠ¶æ€å¤„ç† -------------------
        if not options:
            # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œä¸”æ²¡åœ¨å¯¼å…¥ï¼Œæ˜¾å¼æç¤º
            selected_dataset_name = None
            result_dir = None
        else:
            # ------------------- ä¾§è¾¹æ é€»è¾‘ï¼šæ•°æ®é›†é€‰æ‹©å™¨ -------------------
            # æ•°æ®é›†ä¸‹æ‹‰é€‰æ‹©å™¨
            selected_dataset_name = st.selectbox(
                "é€‰æ‹©å½“å‰æ•°æ®é›†",
                options=options,
                index=0,
                label_visibility="visible",
                key="dataset_selector"
            )
            result_dir = st.session_state.data_sources[selected_dataset_name]
            if "last_dataset_name" not in st.session_state:
                st.session_state.last_dataset_name = selected_dataset_name
            elif selected_dataset_name != st.session_state.last_dataset_name:
                st.session_state.last_dataset_name = selected_dataset_name
                if st.session_state.auto_prewarm:
                    st.session_state.prewarm_pending = True
                    st.session_state.prewarm_mode = "auto"
                    st.session_state.prewarm_notified = False


        # æ•°æ®é›†æ“ä½œå·¥å…·æ  (åˆ é™¤ä¸æ–°å¢)
        col_del, col_add = st.columns(2)
        
        with col_del:
            if selected_dataset_name:
                if st.button("ğŸ—‘ï¸ ç§»é™¤", type="secondary", use_container_width=True):
                    if st.session_state.data_sources.get(selected_dataset_name) == "__UPLOADED__":
                        if 'uploaded_data' in st.session_state:
                            del st.session_state.uploaded_data
                    del st.session_state.data_sources[selected_dataset_name]
                    # é‡ç½®é€‰æ‹©å™¨çŠ¶æ€
                    if "dataset_selector" in st.session_state:
                        del st.session_state.dataset_selector
                    st.rerun()
            else:
                 st.button("ğŸ—‘ï¸ ç§»é™¤", type="secondary", disabled=True, use_container_width=True)
 
        if 'rename_dialog_open' not in st.session_state:
            st.session_state.rename_dialog_open = False
            
        @st.dialog("é‡å‘½åæ•°æ®é›†")
        def rename_dialog(default_name, valid_path):
            new_name = st.text_input("æ˜¾ç¤ºåç§°", value=default_name)
            if st.button("ç¡®è®¤æ·»åŠ ", type="primary", use_container_width=True):
                if new_name.strip():
                    normalized_name = normalize_dataset_name(new_name)
                    normalized_name = ensure_unique_dataset_name(normalized_name, st.session_state.data_sources)
                    st.session_state.data_sources[normalized_name] = valid_path
                    st.session_state.dataset_selector = normalized_name
                    st.session_state.temp_import_path = "" # Clear path
                    st.session_state.rename_dialog_open = False # Close flag
                    
                    st.rerun()
                else:
                    st.error("åç§°ä¸èƒ½ä¸ºç©º")
        
        with col_add:
            is_cloud = utils.is_cloud_environment()
            
            if is_cloud:
                # Cloud: Toggle Button
                btn_label = "âœ–ï¸ å–æ¶ˆ" if st.session_state.show_import else "ğŸ“‚ å¯¼å…¥"
                if st.button(btn_label, type="secondary", use_container_width=True):
                    st.session_state.show_import = not st.session_state.show_import
                    st.rerun()
            else:
                # Local: Direct Browse with Dialog
                if st.button("ğŸ“‚ å¯¼å…¥", type="secondary", use_container_width=True):
                    folder = utils.open_folder_dialog()
                    if folder:
                         # Validate Path immediately
                        valid_path = None
                        if os.path.exists(os.path.join(folder, "predict_result.csv")):
                            valid_path = folder
                        elif os.path.exists(os.path.join(folder, "results", "predict_result.csv")):
                            valid_path = os.path.join(folder, "results")
                        
                        if valid_path:
                            st.session_state.temp_import_path = valid_path
                            st.session_state.rename_dialog_open = True
                            st.rerun()
                        else:
                            st.toast("âŒ ç›®å½•æ— æ•ˆï¼šæœªæ‰¾åˆ° predict_result.csv", icon="ğŸš«")

        # Trigger Dialog if flag is set (Local Only)
        if st.session_state.get('rename_dialog_open') and st.session_state.get('temp_import_path'):
            # Smart Naming: If the selected folder is 'results', use the parent folder name
            raw_basename = os.path.basename(st.session_state.temp_import_path)
            if raw_basename.lower() == "results":
                parent_name = os.path.basename(os.path.dirname(st.session_state.temp_import_path))
                base_name = parent_name
            else:
                base_name = raw_basename
            
            rename_dialog(normalize_dataset_name(base_name), st.session_state.temp_import_path)

        # Cloud Import Logic (Only visible if Cloud mode AND show_import is True)
        if is_cloud and st.session_state.show_import:
             with st.container():
                
                uploaded_files = st.file_uploader(
                    "ä¸Šä¼ æ•°æ®æ–‡ä»¶",
                    type=["csv"],
                    accept_multiple_files=True,
                    help="è¯·ä¸Šä¼  predict_result.csv å’Œ coordinates.csv",
                    key="cloud_uploader",
                    label_visibility="collapsed"
                )
                
                if uploaded_files:
                    oversize_files = [
                        f.name for f in uploaded_files
                        if getattr(f, "size", 0) > MAX_UPLOAD_MB * 1024 * 1024
                    ]
                    if oversize_files:
                        st.error(f"ä¸Šä¼ æ–‡ä»¶è¿‡å¤§ï¼Œå•æ–‡ä»¶ä¸Šé™ {MAX_UPLOAD_MB}MB: {', '.join(oversize_files)}")
                        return
                    file_names = [f.name.lower() for f in uploaded_files]
                    if any("predict" in name for name in file_names):
                        # Auto-generate default name: dataset_1, dataset_2, ...
                        counter = 1
                        while f"dataset_{counter}" in st.session_state.data_sources:
                            counter += 1
                        default_cloud_name = f"dataset_{counter}"
                        
                        new_name = st.text_input("æ•°æ®é›†æ˜¾ç¤ºåç§°", value=default_cloud_name)
                        
                        def on_upload_confirm():
                            if new_name.strip():
                                pdf, cdf, errors, warnings = run_timed(
                                    "upload_parse",
                                    lambda: data_loader.load_from_uploaded_files(
                                        uploaded_files,
                                        use_parallel=st.session_state.parallel_load
                                    ),
                                    {"files": len(uploaded_files), "parallel": st.session_state.parallel_load}
                                )
                                render_messages(errors, warnings)
                                if errors:
                                    return
                                if pdf is not None:
                                    normalized_name = normalize_dataset_name(new_name)
                                    normalized_name = ensure_unique_dataset_name(normalized_name, st.session_state.data_sources)
                                    st.session_state.uploaded_data_cache = {
                                        'predict_df': pdf,
                                        'coords': cdf
                                    }
                                    st.session_state.data_sources[normalized_name] = "__UPLOADED__"
                                    st.session_state.dataset_selector = normalized_name
                                    st.session_state.show_import = False
                                else:
                                    st.toast("âŒ æ•°æ®è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ CSV æ ¼å¼", icon="âŒ")
                            else:
                                st.error("è¯·è¾“å…¥åç§°")
                        
                        st.button("âœ… ç¡®è®¤ä¸Šä¼ ", type="primary", use_container_width=True, on_click=on_upload_confirm)
                    else:
                        st.warning("âš ï¸ å¿…éœ€æ–‡ä»¶ç¼ºå¤±ï¼šè¯·åŠ¡å¿…ä¸Šä¼  `predict_result.csv`")


    # === ä¸»ç•Œé¢å±•ç¤ºåŒº ===
    
    # æ— æ•°æ®åœºæ™¯ï¼šå±•ç¤ºæ¬¢è¿é¡µä¸ç³»ç»Ÿç®€ä»‹
    if result_dir is None:
        # æŒ‡å‘ä¾§è¾¹æ çš„äº¤äº’æŒ‡å¼•
        st.markdown('<div class="sidebar-hint"><i class="fa-solid fa-angles-left" style="font-size:3rem; color:#00f260; filter: drop-shadow(0 0 10px #00f260);"></i></div>', unsafe_allow_html=True)
        
        # é¦–é¡µè§†è§‰æ¸²æŸ“ (åŸºäº Assets å›¾ç‰‡ä¸åŠ¨æ€æ ·å¼)
        banner_base64 = utils.get_base64_image_cached(str(utils.BANNER_PATH))
        banner_src = f"data:image/png;base64,{banner_base64}" if banner_base64 else ""
 
        st.markdown(styles.get_landing_page_html(banner_src), unsafe_allow_html=True)
        return
    

         
    prewarm_future = st.session_state.get("prewarm_future")
    if prewarm_future is not None and prewarm_future.done() and not st.session_state.prewarm_notified:
        prewarm_errors, prewarm_warnings = prewarm_future.result()
        st.session_state.prewarm_notified = True
        render_messages(prewarm_errors, prewarm_warnings)
        if prewarm_errors:
            st.toast("âŒ åå°é¢„çƒ­å¤±è´¥", icon="âŒ")
        else:
            st.toast("âœ… åå°é¢„çƒ­å®Œæˆ", icon="âœ…")

    if st.session_state.prewarm_pending:
        if result_dir and result_dir != "__UPLOADED__":
            if st.session_state.prewarm_mode == "manual":
                st.session_state.prewarm_pending = False
                with st.spinner("æ­£åœ¨é¢„çƒ­ç¼“å­˜..."):
                    prewarm_predict, prewarm_coords, prewarm_errors, prewarm_warnings = run_timed(
                        "prewarm_load",
                        lambda: data_loader.load_results(
                            result_dir,
                            use_parallel=st.session_state.parallel_load,
                            use_binary_cache=st.session_state.binary_cache
                        ),
                        {"parallel": st.session_state.parallel_load, "binary_cache": st.session_state.binary_cache}
                    )
                    render_messages(prewarm_errors, prewarm_warnings)
                    if prewarm_errors:
                        return
                    if st.session_state.prewarm_bg and prewarm_predict is not None and prewarm_coords is not None:
                        bg_cache_key = f"{selected_dataset_name}_bg_img"
                        if bg_cache_key not in st.session_state.figure_cache:
                            bg_img, (xlim, ylim) = run_timed(
                                "prewarm_bg",
                                lambda: utils.get_or_generate_pie_background(
                                    prewarm_predict,
                                    prewarm_coords,
                                    result_dir
                                ),
                                {"points": len(prewarm_predict)}
                            )
                            st.session_state.figure_cache[bg_cache_key] = {"img": bg_img, "xlim": xlim, "ylim": ylim}
                    st.toast("âœ… ç¼“å­˜é¢„çƒ­å®Œæˆ", icon="âœ…")
            else:
                if prewarm_future is None or prewarm_future.done():
                    executor = ThreadPoolExecutor(max_workers=1)
                    st.session_state.prewarm_future = executor.submit(
                        data_loader.prewarm_cache,
                        result_dir,
                        st.session_state.parallel_load,
                        st.session_state.binary_cache
                    )
                    st.session_state.prewarm_pending = False
                    st.toast("â³ å·²å¼€å§‹åå°é¢„çƒ­", icon="â³")
        else:
            st.session_state.prewarm_pending = False
            st.toast("âš ï¸ ä»…æœ¬åœ°æ•°æ®æ”¯æŒé¢„çƒ­ç¼“å­˜", icon="âš ï¸")

    # æœ‰æ•ˆæ•°æ®åœºæ™¯ï¼šæ‰§è¡Œæ•°æ®æµåŠ è½½
    if result_dir == "__UPLOADED__":
        # äº‘ç«¯éƒ¨ç½²åŠ è½½é€»è¾‘ï¼šé€šè¿‡ Session State æ¢å¤
        if 'uploaded_data_cache' in st.session_state:
            predict_df = st.session_state.uploaded_data_cache['predict_df']
            coords = st.session_state.uploaded_data_cache['coords']
            predict_df, coords, errors, warnings = run_timed(
                "data_validate_uploaded",
                lambda: data_loader.validate_dataset(predict_df, coords)
            )
            render_messages(errors, warnings)
            if errors:
                return
        else:
            st.error("âŒ ä¼šè¯è¿‡æœŸï¼šä¸Šä¼ çš„æ•°æ®å·²å¤±æ•ˆï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡ä»¶ã€‚")
            return
    else:
        # æœ¬åœ°å¼€å‘åŠ è½½é€»è¾‘ï¼šé€šè¿‡æ–‡ä»¶ç³»ç»Ÿè¯»å–
        with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®é›†..."):
            predict_df, coords, errors, warnings = run_timed(
                "data_load_local",
                lambda: data_loader.load_results(
                    result_dir,
                    use_parallel=st.session_state.parallel_load,
                    use_binary_cache=st.session_state.binary_cache
                ),
                {"parallel": st.session_state.parallel_load, "binary_cache": st.session_state.binary_cache}
            )
        render_messages(errors, warnings)
        if errors:
            return
    
    if predict_df is not None:
        cell_types = data_loader.get_cell_types(predict_df)
    else:
        st.error("âŒ åŠ è½½å¤±è´¥ï¼šæœªèƒ½è§£æåå·ç§¯ç»“æœæ–‡ä»¶ã€‚")
        st.info("è¯·ç¡®ä¿è¾“å‡ºç›®å½•å®Œæ•´ï¼Œæˆ–å°è¯•é‡æ–°å¯¼å…¥æ•°æ®ã€‚")
        return
    
    # æ ¸å¿ƒæŒ‡æ ‡çœ‹æ¿æ¸²æŸ“
    if predict_df is not None:
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç©ºé—´è§‚æµ‹ä½ç‚¹", f"{len(predict_df):,}")
        with col2:
            st.metric("æ£€æµ‹ç»†èƒç±»å‹", len(cell_types))
        with col3:
            st.metric("ä¸°åº¦æœ€é«˜ç±»å‹", predict_df.mean().idxmax())
        with col4:
            st.metric("å¹³å‡å æ¯”å³°å€¼", f"{predict_df[predict_df.mean().idxmax()].mean():.2%}")
        
        st.divider()
        
        # ========== æ¨¡å—åŒ–å›¾è¡¨è§†å›¾æ¸²æŸ“ ==========
        
        # åˆå§‹åŒ–å›¾è¡¨ç¼“å­˜ç³»ç»Ÿ (åŸºäº Session State ç¡®ä¿åˆ‡æ¢ Tab æ— éœ€é‡ç®—)
        if 'figure_cache' not in st.session_state:
            st.session_state.figure_cache = {}
        
        # å½“å‰æ•°æ®é›†çš„ç¼“å­˜é”®å‰ç¼€
        cache_prefix = f"{selected_dataset_name}_"
        
        perf_mode = get_perf_mode()
        use_lod = perf_mode == "é«˜æ€§èƒ½"
        tab_labels = [
            "ğŸ§© ç©ºé—´ç»„åˆ†å›¾è°±",
            "ğŸ” ä¼˜åŠ¿äºšç¾¤åˆ†å¸ƒ",
            "ğŸ“Š ç»†èƒæ¯”ä¾‹æ¦‚è§ˆ",
            "ğŸŒ¡ï¸ å•ç»†èƒçƒ­åº¦å›¾",
            "ğŸ“‘ åŸå§‹æ•°æ®è¯¦å•"
        ]
        if SHOW_PERF_MONITOR_TAB:
            tab_labels.append("âš™ï¸ æ€§èƒ½ç›‘æ§")
        tabs = st.tabs(tab_labels)
        
        with tabs[0]:
            coords_for_plot = coords
            if "tab1_hover_value" not in st.session_state:
                st.session_state.tab1_hover_value = min(6, len(cell_types))
            
            with st.expander("è®¾ç½®", expanded=False):
                st.slider(
                    "æ‚¬åœè¯¦æƒ…æ•°é‡",
                    3,
                    len(cell_types),
                    st.session_state.tab1_hover_value,
                    key="tab1_hover_value"
                )
            hover_count_tab1 = st.session_state.tab1_hover_value

            if coords_for_plot is not None:
                bg_cache_key = f"{cache_prefix}bg_img"
                if bg_cache_key in st.session_state.figure_cache:
                    cached_bg = st.session_state.figure_cache[bg_cache_key]
                    bg_img = cached_bg['img']
                    xlim, ylim = cached_bg['xlim'], cached_bg['ylim']
                else:
                    progress_bar = st.progress(0, text="ğŸ§ª æ­£åœ¨é€šè¿‡å¹¶è¡Œç®¡é“è®¡ç®—ç©ºé—´é¥¼å›¾è½¨è¿¹...")
                    def update_progress(pct, msg):
                        progress_bar.progress(pct, text=f"â³ {msg}")
                    bg_img, (xlim, ylim) = run_timed(
                        "tab1_bg_generate",
                        lambda: utils.get_or_generate_pie_background(
                            predict_df,
                            coords_for_plot,
                            result_dir,
                            progress_callback=update_progress
                        ),
                        {"points": len(predict_df)}
                    )
                    progress_bar.empty()
                    st.session_state.figure_cache[bg_cache_key] = {'img': bg_img, 'xlim': xlim, 'ylim': ylim}

                plot_predict_df = predict_df
                plot_coords = coords_for_plot
                sampled = False
                if use_lod:
                    plot_predict_df, plot_coords, sampled = utils.apply_lod_sampling(predict_df, coords_for_plot)

                tab1_cache_key = f"{cache_prefix}tab1_{hover_count_tab1}_{'lod' if sampled else 'full'}"
                if tab1_cache_key not in st.session_state.figure_cache:
                    cell_type_color_map = utils.get_color_map_cached(tuple(plot_predict_df.columns.tolist()), plot_predict_df)
                    fig = run_timed(
                        "tab1_render",
                        lambda: utils.generate_plotly_scatter(
                            plot_coords,
                            plot_predict_df,
                            hover_count_tab1,
                            bg_img,
                            (xlim, ylim),
                            cell_type_color_map
                        ),
                        {"points": len(plot_predict_df), "sampled": sampled}
                    )
                    st.session_state.figure_cache[tab1_cache_key] = fig
                else:
                    fig = st.session_state.figure_cache[tab1_cache_key]

                st.plotly_chart(fig, use_container_width=True, config={'scrollZoom': True, 'displaylogo': False, 'responsive': True})
                st.caption("ğŸ’¡ è§†å›¾è¯´æ˜ï¼šèƒŒæ™¯å±‚å±•ç¤ºå„è§‚æµ‹ä½ç‚¹çš„å¤šç»„åˆ†æ„æˆï¼›æ‚¬åœå¯æ¢ç´¢äºšç»†èƒçº§å æ¯”è¯¦æƒ…ã€‚")
                if sampled:
                    st.caption("âš¡ å½“å‰ä¸ºé«˜æ€§èƒ½é‡‡æ ·è§†å›¾")
            else:
                st.warning("âš ï¸ åæ ‡æ•°æ®ç¼ºå¤±æˆ–ä¸å…¼å®¹ï¼šæ— æ³•ç”Ÿæˆç©ºé—´æ‹“æ‰‘å›¾ã€‚")
                 
        with tabs[1]:
            if "tab2_hover_value" not in st.session_state:
                st.session_state.tab2_hover_value = min(6, len(cell_types))
            
            with st.expander("è®¾ç½®", expanded=False):
                st.slider(
                    "æ‚¬åœè¯¦æƒ…æ•°é‡",
                    3,
                    len(cell_types),
                    st.session_state.tab2_hover_value,
                    key="tab2_hover_value"
                )
            hover_count = st.session_state.tab2_hover_value

            if coords_for_plot is not None:
                plot_predict_df = predict_df
                plot_coords = coords_for_plot
                sampled = False
                if use_lod:
                    plot_predict_df, plot_coords, sampled = utils.apply_lod_sampling(predict_df, coords_for_plot)

                tab2_cache_key = f"{cache_prefix}tab2_{hover_count}_{'lod' if sampled else 'full'}"
                if tab2_cache_key not in st.session_state.figure_cache:
                    color_map = utils.get_color_map_cached(tuple(plot_predict_df.columns.tolist()), plot_predict_df)
                    fig = run_timed(
                        "tab2_render",
                        lambda: utils.generate_dominant_scatter(plot_coords, plot_predict_df, hover_count, color_map),
                        {"points": len(plot_predict_df), "sampled": sampled}
                    )
                    st.session_state.figure_cache[tab2_cache_key] = fig
                else:
                    fig = st.session_state.figure_cache[tab2_cache_key]

                st.plotly_chart(fig, use_container_width=True, config={'scrollZoom': True, 'displaylogo': False, 'responsive': True})
                st.caption("ğŸ–±ï¸ äº¤äº’è´´å£«ï¼šé€šè¿‡ç‚¹å‡»å³ä¾§å›¾ä¾‹å¯è¿›è¡Œç»†èƒç±»å‹ç­›é€‰ï¼›åŒå‡»å¯åˆ‡æ¢ç‹¬æ˜¾/å…¨é€‰æ¨¡å¼ã€‚")
                if sampled:
                    st.caption("âš¡ å½“å‰ä¸ºé«˜æ€§èƒ½é‡‡æ ·è§†å›¾")
            else:
                st.warning("âš ï¸ æ•°æ®å¼‚å¸¸ï¼šè¯¥æ•°æ®é›†æ— æ³•è¿›è¡Œä¼˜åŠ¿äºšç¾¤èšç±»æ˜ å°„ã€‚")
        
        with tabs[2]:
            tab3_cache_key = f"{cache_prefix}tab3"

            if tab3_cache_key not in st.session_state.figure_cache:
                fig = run_timed(
                    "tab3_render",
                    lambda: utils.generate_proportion_bar(predict_df),
                    {"types": len(cell_types)}
                )
                st.session_state.figure_cache[tab3_cache_key] = fig
            else:
                fig = st.session_state.figure_cache[tab3_cache_key]

            st.plotly_chart(fig, use_container_width=True)

        with tabs[3]:
            if "tab4_type_value" not in st.session_state:
                st.session_state.tab4_type_value = cell_types[0]
            selected_index = 0
            if st.session_state.tab4_type_value in cell_types:
                selected_index = cell_types.index(st.session_state.tab4_type_value)
            st.selectbox("ğŸ”¬ æ£€ç´¢ç›®æ ‡ç»†èƒäºšç¾¤", cell_types, index=selected_index, key="tab4_type_value")
            selected_type = st.session_state.tab4_type_value

            if coords_for_plot is not None:
                plot_predict_df = predict_df
                plot_coords = coords_for_plot
                sampled = False
                if use_lod:
                    plot_predict_df, plot_coords, sampled = utils.apply_lod_sampling(predict_df, coords_for_plot)

                tab4_cache_key = f"{cache_prefix}tab4_{selected_type}_{'lod' if sampled else 'full'}"
                if tab4_cache_key not in st.session_state.figure_cache:
                    fig = run_timed(
                        "tab4_render",
                        lambda: utils.generate_heatmap(plot_coords, plot_predict_df, selected_type),
                        {"points": len(plot_predict_df), "sampled": sampled}
                    )
                    st.session_state.figure_cache[tab4_cache_key] = fig
                else:
                    fig = st.session_state.figure_cache[tab4_cache_key]

                st.plotly_chart(fig, use_container_width=True, config={'scrollZoom': True, 'displaylogo': False, 'responsive': True})
                if sampled:
                    st.caption("âš¡ å½“å‰ä¸ºé«˜æ€§èƒ½é‡‡æ ·è§†å›¾")
            else:
                st.warning("âš ï¸ æç¤ºï¼šç¼ºå°‘è¯¥æ ·æœ¬çš„ç©ºé—´åæ ‡ã€‚")

        with tabs[4]:
            st.markdown("#### ğŸ“‘ åå·ç§¯é¢„æµ‹åŸå§‹æŒ‡æ ‡çŸ©é˜µ")
            st.dataframe(predict_df, use_container_width=True, height=500)

        if SHOW_PERF_MONITOR_TAB:
            with tabs[-1]:
                st.markdown("#### âš™ï¸ æ€§èƒ½ç›‘æ§")
                with st.expander("æ€§èƒ½è®¾ç½®", expanded=False):
                    if st.button("æ¸…ç©ºæ€§èƒ½è®°å½•", type="secondary", use_container_width=True):
                        st.session_state.perf_metrics = []
                        st.session_state.mem_snapshot = None
                    if st.button("é¢„çƒ­ç¼“å­˜", type="secondary", use_container_width=True):
                        st.session_state.prewarm_pending = True
                        st.session_state.prewarm_mode = "manual"
                        st.session_state.prewarm_notified = False
                    prewarm_future = st.session_state.get("prewarm_future")
                    if prewarm_future is not None and not prewarm_future.done():
                        st.caption("åå°é¢„çƒ­ä¸­â€¦")
                if st.session_state.get("perf_monitor_enabled"):
                    if st.button("é‡‡é›†å†…å­˜å¿«ç…§", type="secondary", use_container_width=False):
                        if not tracemalloc.is_tracing():
                            tracemalloc.start()
                        current, peak = tracemalloc.get_traced_memory()
                        st.session_state.mem_snapshot = {
                            "current_mb": round(current / 1024 / 1024, 2),
                            "peak_mb": round(peak / 1024 / 1024, 2)
                        }

                    metrics_df = pd.DataFrame(st.session_state.perf_metrics)
                    if not metrics_df.empty:
                        st.dataframe(metrics_df.tail(200), use_container_width=True, height=280)
                        summary = (
                            metrics_df
                            .groupby(["mode", "label"])["duration_ms"]
                            .agg(
                                count="count",
                                avg_ms="mean",
                                p95_ms=lambda s: s.quantile(0.95)
                            )
                            .reset_index()
                        )
                        st.dataframe(summary, use_container_width=True, height=260)
                    if st.session_state.mem_snapshot:
                        st.metric("å½“å‰å†…å­˜(MB)", st.session_state.mem_snapshot["current_mb"])
                        st.metric("å³°å€¼å†…å­˜(MB)", st.session_state.mem_snapshot["peak_mb"])
                else:
                    st.info("å¯ç”¨æ€§èƒ½ç›‘æ§åå¯æŸ¥çœ‹æ¸²æŸ“è€—æ—¶ä¸å†…å­˜å¿«ç…§")

if __name__ == "__main__":
    main()
